{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('../data/playground-series-s5e3/train.csv')\n",
    "test_df = pd.read_csv('../data/playground-series-s5e3/test.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "X = train_df.drop(['id', 'rainfall'], axis=1)\n",
    "y = train_df['rainfall']\n",
    "X_test_full = test_df.drop(['id'], axis=1)\n",
    "\n",
    "# Enhanced data preprocessing pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = make_pipeline(\n",
    "    SimpleImputer(strategy='median', add_indicator=True),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "X = preprocessor.fit_transform(X)\n",
    "X_test_full = preprocessor.transform(X_test_full)\n",
    "\n",
    "# Convert to numpy arrays and ensure proper data types\n",
    "X = np.asarray(X, dtype=np.float32)\n",
    "X_test_full = np.asarray(X_test_full, dtype=np.float32)\n",
    "\n",
    "# Detailed data validation with logging\n",
    "def validate_data(data, name):\n",
    "    print(f'\\nValidating {name}...')\n",
    "    print(f'Shape: {data.shape}')\n",
    "    print(f'Memory usage: {data.nbytes / 1024 / 1024:.2f} MB')\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_count = np.isnan(data).sum()\n",
    "    if nan_count > 0:\n",
    "        nan_indices = np.where(np.isnan(data))\n",
    "        print(f'NaN values found in columns: {np.unique(nan_indices[1])}')\n",
    "        print(f'NaN counts per column: {np.isnan(data).sum(axis=0)}')\n",
    "        raise ValueError(\n",
    "            f'Found {nan_count} NaN values in {name} at indices: {nan_indices}'\n",
    "        )\n",
    "    \n",
    "    # Check for infinite values\n",
    "    inf_count = np.isinf(data).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_indices = np.where(np.isinf(data))\n",
    "        print(f'Infinite values found in columns: {np.unique(inf_indices[1])}')\n",
    "        raise ValueError(\n",
    "            f'Found {inf_count} infinite values in {name} at indices: {inf_indices}'\n",
    "        )\n",
    "    \n",
    "    # Check value ranges\n",
    "    print(f'Min values: {np.min(data, axis=0)}')\n",
    "    print(f'Max values: {np.max(data, axis=0)}')\n",
    "    print(f'Mean values: {np.mean(data, axis=0)}')\n",
    "    print(f'Std values: {np.std(data, axis=0)}')\n",
    "    print(f'Validation of {name} completed successfully\\n')\n",
    "\n",
    "validate_data(X, 'training features')\n",
    "validate_data(X_test_full, 'test features')\n",
    "validate_data(y.values.reshape(-1, 1), 'training labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward stepwise feature selection\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Initialize models\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "xgb = XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Create feature selection pipeline\n",
    "selector = SequentialFeatureSelector(\n",
    "    xgb,\n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit selector on training data\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform datasets\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)\n",
    "X_test_full = selector.transform(X_test_full)\n",
    "\n",
    "print(f'Selected {X_train.shape[1]} features out of {selector.n_features_in_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross validation for both models\n",
    "print('Logistic Regression Cross-validation:')\n",
    "log_reg_cv = cross_val_score(log_reg, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(f'CV scores: {log_reg_cv}')\n",
    "print(f'Mean CV accuracy: {log_reg_cv.mean():.4f}')\n",
    "\n",
    "print('\\nXGBoost Cross-validation:')\n",
    "xgb_cv = cross_val_score(xgb, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(f'CV scores: {xgb_cv}')\n",
    "print(f'Mean CV accuracy: {xgb_cv.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on full training set\n",
    "log_reg.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "log_reg_pred = log_reg.predict(X_test)\n",
    "log_reg_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "xgb_pred_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Make predictions on full test set\n",
    "test_pred_log_reg = log_reg.predict(X_test_full)\n",
    "test_pred_xgb = xgb.predict(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "def evaluate_model(name, y_true, y_pred, y_pred_proba):\n",
    "    print(f'\\n{name} Validation Set Metrics:')\n",
    "    print(f'Accuracy: {accuracy_score(y_true, y_pred):.4f}')\n",
    "    print(f'Precision: {precision_score(y_true, y_pred):.4f}')\n",
    "    print(f'Recall: {recall_score(y_true, y_pred):.4f}')\n",
    "    print(f'F1 Score: {f1_score(y_true, y_pred):.4f}')\n",
    "    print(f'ROC AUC: {roc_auc_score(y_true, y_pred_proba):.4f}')\n",
    "    \n",
    "    # Calculate AUC using integration\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    print(f'Calculated AUC: {auc:.4f}')\n",
    "    return fpr, tpr, auc\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "log_reg_fpr, log_reg_tpr, log_reg_auc = evaluate_model(\n",
    "    'Logistic Regression', y_test, log_reg_pred, log_reg_pred_proba\n",
    ")\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_fpr, xgb_tpr, xgb_auc = evaluate_model(\n",
    "    'XGBoost', y_test, xgb_pred, xgb_pred_proba\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "cm_log_reg = confusion_matrix(y_test, log_reg_pred)\n",
    "sns.heatmap(cm_log_reg, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_title('Logistic Regression Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# XGBoost confusion matrix\n",
    "cm_xgb = confusion_matrix(y_test, xgb_pred)\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
    "ax2.set_title('XGBoost Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(log_reg_fpr, log_reg_tpr, label=f'Logistic Regression (AUC = {log_reg_auc:.4f})')\n",
    "plt.plot(xgb_fpr, xgb_tpr, label=f'XGBoost (AUC = {xgb_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
